{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4ff5945",
   "metadata": {},
   "source": [
    "## 1. Importar Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c6461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898725f",
   "metadata": {},
   "source": [
    "## 2. Cargar Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cfed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "print(f\"Modelo: {os.getenv('MODEL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a483d89",
   "metadata": {},
   "source": [
    "## 3. Configurar el Cliente de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_APIKEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494f69c",
   "metadata": {},
   "source": [
    "## 4. Definir Par√°metros de la Conversaci√≥n\n",
    "\n",
    "Definimos:\n",
    "- El tema a debatir\n",
    "- Las personalidades de cada asistente\n",
    "- N√∫mero de iteraciones (turnos de conversaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tema_a_tratar = \"¬øQuien es mejor equipo: el Real Madrid o el FC Barcelona?\"\n",
    "personalidad_asistente_1 = \"Soy del Real Madrid y tengo muy mala leche. Soy maleducado y borde. No soporto al FC Barcelona.\"\n",
    "personalidad_asistente_2 = \"Soy del FC Barcelona y me gusta mucho pinchar a los del Real Madrid. Soy sarc√°stico y bromista. Muchas veces soy insoportable.\"\n",
    "iteraciones = 5  # N√∫mero de turnos de conversaci√≥n\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFIGURACI√ìN DE LA CONVERSACI√ìN\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTema: {tema_a_tratar}\")\n",
    "print(f\"\\nAsistente 1 (Real Madrid): {personalidad_asistente_1}\")\n",
    "print(f\"\\nAsistente 2 (FC Barcelona): {personalidad_asistente_2}\")\n",
    "print(f\"\\nIteraciones: {iteraciones}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497dd2e8",
   "metadata": {},
   "source": [
    "## 5. Inicializar Historiales de Conversaci√≥n\n",
    "\n",
    "Cada asistente tiene su propio historial de mensajes. Ambos comienzan con un mensaje de sistema que define su personalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ea2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_1 = [\n",
    "    {\"role\": \"system\", \"content\": personalidad_asistente_1},\n",
    "]\n",
    "\n",
    "messages_2 = [\n",
    "    {\"role\": \"system\", \"content\": personalidad_asistente_2},\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Historiales de conversaci√≥n inicializados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb28a119",
   "metadata": {},
   "source": [
    "## 6. Iniciar la Conversaci√≥n\n",
    "\n",
    "El Asistente 2 (Barcelona) recibe el tema como pregunta inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c040f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = tema_a_tratar\n",
    "messages_2.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "model = os.getenv(\"MODEL\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INICIO DE LA CONVERSACI√ìN\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "print(f\"Tema inicial: {question}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3662145e",
   "metadata": {},
   "source": [
    "## 7. Loop de Conversaci√≥n\n",
    "\n",
    "En cada iteraci√≥n:\n",
    "1. Asistente 2 responde\n",
    "2. Su respuesta se guarda y se pasa al Asistente 1\n",
    "3. Asistente 1 responde\n",
    "4. Su respuesta se guarda y se pasa de vuelta al Asistente 2\n",
    "\n",
    "Esto crea un ciclo de debate continuo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iteraciones):\n",
    "    print(f\"\\n{'‚îÄ' * 80}\")\n",
    "    print(f\"TURNO {i+1}\")\n",
    "    print(f\"{'‚îÄ' * 80}\\n\")\n",
    "    \n",
    "    # Asistente 2 (Barcelona) responde\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages_2\n",
    "    )\n",
    "    question = response.choices[0].message.content\n",
    "    messages_2.append({\"role\": \"assistant\", \"content\": question})\n",
    "    messages_1.append({\"role\": \"user\", \"content\": question})\n",
    "    print(f\"üîµ Asistente 2 (Barcelona): {question}\")\n",
    "\n",
    "    # Asistente 1 (Madrid) responde\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages_1\n",
    "    )\n",
    "    question = response.choices[0].message.content\n",
    "    messages_1.append({\"role\": \"assistant\", \"content\": question})\n",
    "    messages_2.append({\"role\": \"user\", \"content\": question})\n",
    "    print(f\"\\n‚ö™ Asistente 1 (Madrid): {question}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIN DE LA CONVERSACI√ìN\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63f2ae",
   "metadata": {},
   "source": [
    "## 8. Estad√≠sticas de la Conversaci√≥n\n",
    "\n",
    "Veamos cu√°ntos mensajes se intercambiaron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä ESTAD√çSTICAS\")\n",
    "print(f\"Total de mensajes Asistente 1: {len(messages_1)}\")\n",
    "print(f\"Total de mensajes Asistente 2: {len(messages_2)}\")\n",
    "print(f\"Total de intercambios: {iteraciones * 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dd693",
   "metadata": {},
   "source": [
    "## 9. Ver Historial Completo (Opcional)\n",
    "\n",
    "Si quieres revisar todo el historial de uno de los asistentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe190d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìú HISTORIAL COMPLETO - ASISTENTE 1 (MADRID)\\n\")\n",
    "for i, msg in enumerate(messages_1[1:], 1):  # Omitimos el system prompt\n",
    "    role = \"ü§ñ Asistente\" if msg['role'] == 'assistant' else \"üë§ Usuario\"\n",
    "    print(f\"{i}. {role}: {msg['content'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171af6ac",
   "metadata": {},
   "source": [
    "## Conclusi√≥n\n",
    "\n",
    "Este ejemplo demuestra:\n",
    "\n",
    "### 1. **Conversaci√≥n Multi-Agente**\n",
    "- Dos instancias independientes del modelo\n",
    "- Cada una mantiene su propio contexto y personalidad\n",
    "- Los mensajes fluyen de uno a otro\n",
    "\n",
    "### 2. **Gesti√≥n de Estado**\n",
    "- Historiales de conversaci√≥n separados\n",
    "- Cada asistente ve el historial desde su perspectiva\n",
    "- Consistencia de personalidad a lo largo de la conversaci√≥n\n",
    "\n",
    "### 3. **System Prompts Efectivos**\n",
    "- Definici√≥n clara de personalidad y comportamiento\n",
    "- Los modelos mantienen su rol consistentemente\n",
    "- Genera conversaciones naturales y entretenidas\n",
    "\n",
    "### Aplicaciones Pr√°cticas:\n",
    "- Simulaciones de debates\n",
    "- Entrenamiento de modelos de di√°logo\n",
    "- Generaci√≥n de contenido conversacional\n",
    "- Testing de diferentes estrategias de conversaci√≥n\n",
    "- Juegos de rol y entretenimiento\n",
    "\n",
    "### Advertencia:\n",
    "Este es un ejemplo recreativo. En producci√≥n, considera:\n",
    "- L√≠mites de iteraciones para evitar bucles infinitos\n",
    "- Moderaci√≥n de contenido\n",
    "- Costos de API (cada iteraci√≥n hace 2 llamadas)\n",
    "- Timeouts y manejo de errores"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
